{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "Credits:\n",
    "\n",
    "The following code is my take on the tutorial from [fast.ai](https://github.com/fastai/courses/blob/master/deeplearning2/neural-style.ipynb) and the pytorch implementation from [sunshineatnoon](https://github.com/sunshineatnoon/Paper-Implementations/tree/master/NeuralSytleTransfer). This follows closely the articles [Neural Style Transfer]() and \n",
    "\n",
    "For the images the links are [van gogh self portrait](), [starry night]() and [pixelart](https://www.pinterest.fr/pin/726979564822222007/)\n",
    "\n",
    "All links where functional at 16/04/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import scipy\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import imageio\n",
    "import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose the shape for our images, and set the paths for the different images we are going to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = (400,400,3)\n",
    "content_filename = \"images/me.jpg\"\n",
    "vangogh_filename = \"images/VanGogh.jpg\"\n",
    "retro_filename = \"images/pixelart.jpeg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our network from the weights available in keras. Here I use VGG16 as it is the most used for this application. I also tried using Resnet50 and InceptionV2 but the results were not as good.\n",
    "\n",
    "In the end we print the layers so that we can choose which ones we are going to use. The original paper suggests that using layers at the start for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model layers:\n",
      "input_1\n",
      "block1_conv1\n",
      "block1_conv2\n",
      "block1_pool\n",
      "block2_conv1\n",
      "block2_conv2\n",
      "block2_pool\n",
      "block3_conv1\n",
      "block3_conv2\n",
      "block3_conv3\n",
      "block3_pool\n",
      "block4_conv1\n",
      "block4_conv2\n",
      "block4_conv3\n",
      "block4_pool\n",
      "block5_conv1\n",
      "block5_conv2\n",
      "block5_conv3\n",
      "block5_pool\n"
     ]
    }
   ],
   "source": [
    "network_class = keras.applications.vgg16\n",
    "model = network_class.VGG16(include_top=False,input_shape=shape)\n",
    "\n",
    "print(\"Model layers:\")\n",
    "for a in model.layers:\n",
    "    print(a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_layers = [model.get_layer('block{}_conv1'.format(o)).output for o in range(1,6)]\n",
    "content_layers = [model.get_layer('block{}_conv1'.format(o)).output for o in range(1,6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess = network_class.preprocess_input\n",
    "\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=np.float32) #Extracted from keras.applications.imagenet_utils\n",
    "\n",
    "def deprocess(processed_image):\n",
    "    return np.clip(processed_image[:, :, ::-1] + mean, 0, 255)/255\n",
    "\n",
    "def load_image(filename,shape):\n",
    "    image = plt.imread(filename) \n",
    "    if image.shape[2] == 4:\n",
    "        image = image[:,:,:3]\n",
    "    resized = skimage.transform.resize(image,shape,mode=\"constant\").astype(np.float32) * 255 \n",
    "    return resized,preprocess(resized.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pixelated_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-06c0a6158511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvangogh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvangogh_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvangogh_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpixelated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpixelated_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixelated_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pixelated_filename' is not defined"
     ]
    }
   ],
   "source": [
    "content,content_preprocessed = load_image(content_filename,shape)\n",
    "vangogh,vangogh_preprocessed = load_image(vangogh_filename,shape)\n",
    "pixelated,pixelated_preprocessed = load_image(pixelated_filename,shape)\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(20,10))\n",
    "ax[0].imshow(content/255)\n",
    "ax[1].imshow(vangogh/255)                      \n",
    "ax[2].imshow(pixelated/255)                      \n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(20,10))\n",
    "ax[0].imshow(deprocess(content_preprocessed))\n",
    "ax[1].imshow(deprocess(vangogh_night_preprocessed))                      \n",
    "ax[2].imshow(deprocess(pixelated_preprocessed))                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    def __init__(self, f, shp): self.f, self.shp = f, shp\n",
    "        \n",
    "    def loss(self, x):\n",
    "        loss_, self.grad_values = self.f([x.reshape(self.shp)])\n",
    "        return loss_.astype(np.float64)\n",
    "\n",
    "    def grads(self, x): return self.grad_values.flatten().astype(np.float64)\n",
    "    \n",
    "def mse(layer,target):\n",
    "    return K.mean((layer-target)**2)\n",
    "\n",
    "def solve_image(eval_obj, niter, x,name,iq):\n",
    "    for i in tqdm.tqdm(list(range(niter))):\n",
    "        x, min_val, info = scipy.optimize.fmin_l_bfgs_b(eval_obj.loss, x.flatten(),\n",
    "                                         fprime=eval_obj.grads, maxfun=20)\n",
    "        x = np.clip(x, -127,127)\n",
    "        print('Current loss value:', min_val)\n",
    "        imageio.imwrite(\"results/{}_at_iteration_{}.png\".format(name,i), \n",
    "                        (deprocess(join_yiq_to_bgr(x.reshape(shape),iq))*255).astype(np.uint8))\n",
    "    return x\n",
    "\n",
    "def gram_matrix(x):\n",
    "    # We want each row to be a channel, and the columns to be flattened x,y locations\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    # The dot product of this with its transpose shows the correlation \n",
    "    # between each pair of channels\n",
    "    the_dot = K.dot(features, K.transpose(features))\n",
    "    tensor = tf.cast(K.prod(x.get_shape()[1:3]),tf.float32)\n",
    "    num_elems = tensor\n",
    "    return the_dot / num_elems\n",
    "\n",
    "def style_loss(x, targ): \n",
    "    return mse(gram_matrix(x), gram_matrix(targ))\n",
    "\n",
    "def style_transfer(input_content,input_style,content_multiplier,name,iq): \n",
    "    input_content = np.expand_dims(input_content,0)\n",
    "    input_style = np.expand_dims(input_style,0)\n",
    "    \n",
    "    content_model = keras.Model(model.input, content_layers)\n",
    "    style_model = keras.Model(model.input, style_layers)\n",
    "\n",
    "    content_targets = [K.variable(output) for output in content_model.predict(input_content)]\n",
    "    style_targets = [K.variable(output) for output in style_model.predict(input_style)]    \n",
    "    \n",
    "    style_losses = [style_loss(layer[0], target[0]) for layer,target in zip(style_layers, style_targets)]\n",
    "    content_losses = [mse(layer, target) for layer,target in zip(content_layers, content_targets)]\n",
    "\n",
    "    _style_loss = K.mean(K.stack(style_losses))\n",
    "    content_loss = K.mean(K.stack(content_losses))\n",
    "    \n",
    "    loss = content_multiplier*content_loss + _style_loss\n",
    "    \n",
    "    grads = K.gradients(loss, model.input)\n",
    "    fn = K.function([model.input], [loss]+grads)\n",
    "    evaluator = Evaluator(fn, input_content.shape)\n",
    "    \n",
    "    x = input_content.copy()#np.random.uniform(-2.5, 2.5, [1]+list(shape))/100\n",
    "    x = solve_image(evaluator, 5, x,name,iq)\n",
    "    x = x.reshape(shape)\n",
    "\n",
    "    fig,ax = plt.subplots(1,3,figsize=(20,10))\n",
    "    ax[0].imshow(deprocess(join_yiq_to_bgr(input_content[0],iq)))\n",
    "    ax[1].imshow(deprocess(join_yiq_to_bgr(input_style[0],iq)))                      \n",
    "    ax[2].imshow(deprocess(join_yiq_to_bgr(x,iq)))       \n",
    "    \n",
    "    del evaluator, fn, grads, loss, content_targets,style_targets,content_model,style_model\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bgr_to_yiq(x):\n",
    "    transform = np.asarray([[0.114, 0.587, 0.299], [-0.322, -0.274, 0.596], [0.312, -0.523, 0.211]], dtype=np.float32)\n",
    "    h, w, c = x.shape\n",
    "    x = x.transpose((2, 0, 1)).reshape((c, -1))\n",
    "    x = transform.dot(x)\n",
    "    return x.reshape((c, h, w)).transpose((1, 2, 0))\n",
    "\n",
    "def yiq_to_bgr(x):\n",
    "    transform = np.asarray([[1, -1.106, 1.703], [1, -0.272, -0.647], [1, 0.956, 0.621]], dtype=np.float32)\n",
    "    h, w, c = x.shape\n",
    "    x = x.transpose((2, 0, 1)).reshape((c, -1))\n",
    "    x = transform.dot(x)\n",
    "    return x.reshape((c, h, w)).transpose((1, 2, 0))\n",
    "\n",
    "def split_bgr_to_yiq(x):\n",
    "    x = bgr_to_yiq(x)\n",
    "    y = x[:,:,0:1]\n",
    "    iq = x[:,:,1:]\n",
    "    return np.repeat(y, 3, axis=2), iq\n",
    "\n",
    "def join_yiq_to_bgr(y, iq):\n",
    "    y = bgr_to_yiq(y)[:,:,0:1]\n",
    "    return yiq_to_bgr(np.concatenate((y, iq), axis=2))\n",
    "\n",
    "def luminance_transfer(x,y):\n",
    "    # x: style, y:content\n",
    "    x_l, x_iq = split_bgr_to_yiq(x) # 1x3x512x512\n",
    "    y_l, y_iq = split_bgr_to_yiq(y)\n",
    "\n",
    "    x_l_mean = np.mean(x_l)\n",
    "    y_l_mean = np.mean(y_l)\n",
    "    x_l_std = np.std(x_l)\n",
    "    y_l_std = np.std(y_l)\n",
    "\n",
    "    x_l = (y_l_std/x_l_std)*(x_l - x_l_mean) + y_l_mean\n",
    "    return x_l, y_l, y_iq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vangogh_luminance, content_luminance_vangogh, vangogh_iq = luminance_transfer(vangogh_preprocessed,content_preprocessed)\n",
    "\n",
    "\n",
    "me_vangogh = style_transfer(content_luminance_vangogh,vangogh_luminance,1,\"starry_night\",vangogh_iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outImg = join_yiq_to_bgr(me_vangogh,vangogh_iq)\n",
    "deprocessed = deprocess(outImg)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(deprocessed)\n",
    "deprocessed.min(),deprocessed.max()\n",
    "imageio.imwrite(\"results/me_gogh.png\",(deprocessed*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelated_luminance, content_luminance_pixelated, pixelated_iq = luminance_transfer(pixelated_preprocessed,content_preprocessed)\n",
    "\n",
    "\n",
    "me_pixelated = style_transfer(content_luminance_pixelated,pixelated_luminance,100,\"me_pixel\",pixelated_iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AI]",
   "language": "python",
   "name": "conda-env-AI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
